"""
T√¢che 4: D√©tection de communaut√©s utilisant l'algorithme de propagation d'√©tiquettes (LPA)

Ce script impl√©mente la d√©tection de communaut√©s dans le graphe de similarit√© des prot√©ines en utilisant 
l'algorithme de propagation d'√©tiquettes (LPA) de Neo4j Graph Data Science (GDS).

L'algorithme LPA identifie des communaut√©s de prot√©ines qui sont dens√©ment connect√©es
par des relations de similarit√©, ce qui peut r√©v√©ler des groupes fonctionnels de prot√©ines
ou des familles de prot√©ines avec des architectures de domaines similaires.

Fonctionnalit√©s :
- D√©tection de communaut√©s utilisant LPA avec des param√®tres configurables
- Analyse et statistiques des communaut√©s
- Exportation de la visualisation des communaut√©s d√©tect√©es
- Comparaison de diff√©rentes configurations LPA
"""

import os
import json
import time
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, Counter
from neo4j import GraphDatabase, exceptions


class ProteinCommunityDetector:
    """
    Gestionnaire de d√©tection de communaut√©s pour les graphes de similarit√© des prot√©ines utilisant Neo4j GDS LPA
    """
    
    def __init__(self, neo4j_uri: str = None, user: str = None, password: str = None):
        """
        Initialiser la connexion Neo4j pour la d√©tection de communaut√©s
        
        Args:
            neo4j_uri: Cha√Æne de connexion Neo4j
            user: Nom d'utilisateur Neo4j  
            password: Mot de passe Neo4j
        """
        self.neo4j_uri = neo4j_uri or os.environ.get("NEO4J_URI", "bolt://neo4j:7687")
        self.user = user or os.environ.get("NEO4J_USER", "neo4j")
        self.password = password or os.environ.get("NEO4J_PASSWORD", "password")
        self.driver = None
        
        # Nom de la projection de graphe pour GDS
        self.graph_name = "protein_similarity_graph"
        
    def connect(self):
        """√âtablir la connexion √† Neo4j"""
        try:
            self.driver = GraphDatabase.driver(self.neo4j_uri, auth=(self.user, self.password))
            # Tester la connexion
            with self.driver.session() as session:
                session.run("RETURN 1")
            print(f"‚úÖ Connect√© √† Neo4j √† {self.neo4j_uri}")
            
            # V√©rifier si GDS est disponible
            self._check_gds_availability()
            
        except exceptions.ServiceUnavailable as e:
            print(f"‚ùå Erreur de connexion √† Neo4j : {e}")
            raise
    
    def disconnect(self):
        """Fermer la connexion Neo4j"""
        if self.driver:
            self.driver.close()
            print("üîå D√©connect√© de Neo4j")
    
    def _check_gds_availability(self):
        """V√©rifier si la biblioth√®que Neo4j Graph Data Science est disponible"""
        try:
            with self.driver.session() as session:
                result = session.run("RETURN gds.version() AS version")
                record = result.single()
                if record:
                    print(f"‚úÖ Neo4j GDS disponible - Version: {record['version']}")
                    return True
        except Exception as e:
            print(f"‚ùå Neo4j GDS non disponible : {e}")
            print("Veuillez installer le plugin Neo4j Graph Data Science")
            raise Exception("Plugin GDS requis pour la d√©tection de communaut√©s")
    
    def create_graph_projection(self, 
                              relationship_weight_property: str = "jaccard_weight", min_jaccard_weight: float = 0.1) -> bool:
        """
        Cr√©er une projection de graphe pour les algorithmes GDS
        
        Args:
            relationship_weight_property: Nom de la propri√©t√© pour les poids des relations
            
        Returns:
            Vrai si la projection a √©t√© cr√©√©e avec succ√®s
        """
        try:
            with self.driver.session() as session:
                # D'abord, supprimer la projection existante si elle existe
                drop_query = f"""
                CALL gds.graph.exists('{self.graph_name}') YIELD exists
                WITH exists
                WHERE exists
                CALL gds.graph.drop('{self.graph_name}') YIELD graphName
                RETURN graphName
                """
                session.run(drop_query)
                
                # Cr√©er une nouvelle projection
                projection_query = f"""
                CALL gds.graph.project(
                    '{self.graph_name}',
                    'Protein',
                    {{
                        SIMILAR: {{
                            properties: ['{relationship_weight_property}', 'shared_domains', 'union_domains']
                        }}
                    }}
                )
                YIELD graphName, nodeCount, relationshipCount
                """
                
                result = session.run(projection_query)
                record = result.single()
                
                if record:
                    print(f"‚úÖ Projection de graphe '{record['graphName']}' cr√©√©e :")
                    print(f"   - N≈ìuds : {record['nodeCount']}")
                    print(f"   - Relations : {record['relationshipCount']}")
                    return True
                else:
                    print("‚ùå √âchec de la cr√©ation de la projection de graphe")
                    return False
                    
        except Exception as e:
            print(f"‚ùå Erreur lors de la cr√©ation de la projection de graphe : {e}")
            return False
    
    def estimate_lpa_memory(self, **lpa_config) -> Dict[str, Any]:
        """
        Estimer les besoins en m√©moire pour l'algorithme LPA
        
        Args:
            **lpa_config: Param√®tres de configuration LPA
            
        Returns:
            R√©sultats de l'estimation de la m√©moire
        """
        try:
            with self.driver.session() as session:
                query = f"""
                CALL gds.labelPropagation.write.estimate('{self.graph_name}', 
                    {{writeProperty: 'community'}})
                YIELD nodeCount, relationshipCount, bytesMin, bytesMax, requiredMemory
                """
                
                result = session.run(query)
                record = result.single()
                
                if record:
                    estimation = {
                        'nodeCount': record['nodeCount'],
                        'relationshipCount': record['relationshipCount'],
                        'bytesMin': record['bytesMin'],
                        'bytesMax': record['bytesMax'],
                        'requiredMemory': record['requiredMemory']
                    }
                    
                    print(f"üìä Estimation de la m√©moire LPA :")
                    for key, value in estimation.items():
                        print(f"   {key}: {value}")
                    
                    return estimation
                else:
                    print("‚ùå √âchec de l'estimation de la m√©moire")
                    return {}
                    
        except Exception as e:
            print(f"‚ùå Erreur lors de l'estimation de la m√©moire : {e}")
            return {}
    
    def run_lpa_community_detection(self,
                                   max_iterations: int = 10,
                                   relationship_weight_property: str = "jaccard_weight",
                                   min_community_size: int = 2,
                                   consecutive_ids: bool = True) -> Dict[str, Any]:
        """
        Ex√©cuter l'algorithme de propagation d'√©tiquettes pour la d√©tection de communaut√©s
        
        Args:
            max_iterations: Nombre maximum d'it√©rations
            relationship_weight_property: Propri√©t√© √† utiliser comme poids des relations
            min_community_size: Taille minimale des communaut√©s √† retourner
            consecutive_ids: Indique si les IDs de communaut√© doivent √™tre cons√©cutifs

        Returns:
            R√©sultats et statistiques de l'algorithme
        """
        config = {
            'maxIterations': max_iterations,
            'relationshipWeightProperty': relationship_weight_property,
            'minCommunitySize': min_community_size,
            'consecutiveIds': consecutive_ids,
            'writeProperty': 'community_id'
        }
        
        try:
            with self.driver.session() as session:
                print(f"üöÄ Ex√©cution de l'algorithme de propagation d'√©tiquettes...")
                print(f"   Configuration: {config}")
                
                
                query = f"""
                CALL gds.labelPropagation.write('{self.graph_name}', $config)
                YIELD communityCount, ranIterations, didConverge, 
                        preProcessingMillis, computeMillis, writeMillis
                """
                
                result = session.run(query, config=config)
                
                # Get summary results
                record = result.single()
                if record:
                    results = {
                        'communityCount': record.get('communityCount', 0),
                        'ranIterations': record.get('ranIterations', 0),
                        'didConverge': record.get('didConverge', False),
                        'preProcessingMillis': record.get('preProcessingMillis', 0),
                        'computeMillis': record.get('computeMillis', 0),
                        'writeMillis': record.get('writeMillis', 0),
                    }
                else:
                    print("‚ùå Pas de r√©sultats retourn√©s par LPA")
                    return {}
                
                # R√©sum√© des r√©sultats
                print(f"‚úÖ LPA termin√© avec succ√®s :")
                print(f"   - Communaut√©s trouv√©es : {results.get('communityCount', 'N/A')}")
                if 'ranIterations' in results:
                    print(f"   - It√©rations : {results['ranIterations']}")
                    print(f"   - Converg√© : {results['didConverge']}")
                    print(f"   - Temps de calcul : {results.get('computeMillis', 0)}ms")
                
                return results
                
        except Exception as e:
            print(f"‚ùå Erreur lors de l'ex√©cution de LPA : {e}")
            return {}
    
    def analyze_communities(self) -> Dict[str, Any]:
        """
        Analyser les communaut√©s d√©tect√©es et leurs propri√©t√©s
        
        Returns:
            Analyse d√©taill√©e des communaut√©s
        """
        try:
            with self.driver.session() as session:
                # Obtenir les statistiques des communaut√©s
                stats_query = """
                MATCH (p:Protein)
                WHERE p.community_id IS NOT NULL
                WITH p.community_id AS communityId, collect(p) AS proteins
                RETURN communityId,
                       size(proteins) AS size,
                       proteins
                ORDER BY size DESC
                """
                
                result = session.run(stats_query)
                communities = []
                
                for record in result:
                    community_id = record['communityId']
                    size = record['size']
                    proteins = record['proteins']
                    
                    # Analyser la composition de la communaut√©
                    labeled_count = sum(1 for p in proteins if p.get('is_labelled', False))
                    unlabeled_count = size - labeled_count
                    
                    # Obtenir les num√©ros EC dans cette communaut√©
                    ec_numbers = set()
                    avg_length = 0
                    organisms = set()
                    
                    for protein in proteins:
                        if protein.get('ec_numbers'):
                            ec_numbers.update(protein['ec_numbers'])
                        if protein.get('length'):
                            avg_length += protein['length']
                        if protein.get('organism'):
                            organisms.add(protein['organism'])
                    
                    avg_length = avg_length / size if size > 0 else 0
                    
                    community_info = {
                        'community_id': community_id,
                        'size': size,
                        'labeled_proteins': labeled_count,
                        'unlabeled_proteins': unlabeled_count,
                        'labeling_rate': labeled_count / size if size > 0 else 0,
                        'unique_ec_numbers': len(ec_numbers),
                        'ec_numbers': list(ec_numbers),
                        'avg_sequence_length': round(avg_length, 1),
                        'unique_organisms': len(organisms),
                        'sample_proteins': [
                            {
                                'uniprot_id': p.get('uniprot_id', 'N/A'),
                                'entry_name': p.get('entry_name', 'N/A'),
                                'ec_numbers': p.get('ec_numbers', []),
                                'length': p.get('length', 0),
                                'is_labelled': p.get('is_labelled', False)
                            }
                            for p in proteins[:20]  # 20 √©chantillons de prot√©ines
                        ]
                    }
                    
                    communities.append(community_info)
                
                # Statistiques globales
                total_proteins = sum(c['size'] for c in communities)
                total_labeled = sum(c['labeled_proteins'] for c in communities)
                
                analysis = {
                    'total_communities': len(communities),
                    'total_proteins_in_communities': total_proteins,
                    'total_labeled_in_communities': total_labeled,
                    'overall_labeling_rate': total_labeled / total_proteins if total_proteins > 0 else 0,
                    'largest_community_size': max((c['size'] for c in communities), default=0),
                    'smallest_community_size': min((c['size'] for c in communities), default=0),
                    'avg_community_size': total_proteins / len(communities) if len(communities) > 0 else 0,
                    'communities': communities
                }
                
                print(f"üìà R√©sultats de l'analyse des communaut√©s :")
                print(f"   - Total communaut√©s : {analysis['total_communities']}")
                print(f"   - Prot√©ines dans les communaut√©s : {analysis['total_proteins_in_communities']}")
                print(f"   - Taux global d'√©tiquetage : {analysis['overall_labeling_rate']:.2%}")
                print(f"   - Plage de taille des communaut√©s : {analysis['smallest_community_size']}-{analysis['largest_community_size']}")
                
                return analysis
                
        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse des communaut√©s : {e}")
            return {}
    
    def get_community_proteins(self, community_id: int) -> List[Dict[str, Any]]:
        """
        Obtenir toutes les prot√©ines d'une communaut√© sp√©cifique
        
        Args:
            community_id: ID de la communaut√©
            
        Returns:
            Liste des prot√©ines dans la communaut√©
        """
        try:
            with self.driver.session() as session:
                query = """
                MATCH (p:Protein {community_id: $community_id})
                RETURN p.uniprot_id AS uniprot_id,
                       p.entry_name AS entry_name,
                       p.is_labelled AS is_labelled,
                       p.length AS length,
                       p.ec_numbers AS ec_numbers,
                       p.organism AS organism
                ORDER BY p.uniprot_id
                """
                
                result = session.run(query, community_id=community_id)
                proteins = [dict(record) for record in result]
                
                print(f"‚úÖ {len(proteins)} prot√©ines de la communaut√© {community_id}")
                return proteins
                
        except Exception as e:
            print(f"‚ùå Erreur lors de l'obtention des prot√©ines de la communaut√© : {e}")
            return []
    
    def cleanup_projection(self):
        """Supprimer la projection de graphe GDS"""
        try:
            with self.driver.session() as session:
                query = f"""
                CALL gds.graph.exists('{self.graph_name}') YIELD exists
                WITH exists
                WHERE exists
                CALL gds.graph.drop('{self.graph_name}') YIELD graphName
                RETURN graphName
                """
                result = session.run(query)
                record = result.single()
                if record:
                    print(f"üßπ Projection de graphe nettoy√©e : {record['graphName']}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du nettoyage de la projection : {e}")

    def create_indexes(self):
        """Cr√©er un index pour acc√©l√©rer les recherches par communaut√©"""
        try:
            with self.driver.session() as session:
                # Cr√©ation d'un index sur community_id
                session.run("CREATE INDEX protein_community IF NOT EXISTS FOR (p:Protein) ON (p.community_id)")
                print("‚úÖ Index sur 'community_id' v√©rifi√©/cr√©√©.")
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible de cr√©er l'index : {e}")
        
    def update_ec_numbers_weighted(self, threshold: float = 0.3):
        """
        Mise √† jour avec SEUIL : Ne propage que les EC pr√©sents chez au moins X% 
        des membres √©tiquet√©s de la communaut√©.
        
        Args:
            threshold: Le pourcentage minimum de pr√©sence requis (0.3 = 30%)
        """
        print(f"üîÑ D√©but de la propagation pond√©r√©e (Seuil: {threshold:.0%})...")
        
        query = """
        CALL apoc.periodic.iterate(
            // Identifie les communaut√©s √† traiter
            "MATCH (p:Protein) 
             WHERE p.community_id IS NOT NULL 
             RETURN DISTINCT p.community_id as cid",
            
            // Traite une communaut√© √† la fois avec calcul de fr√©quence
            "MATCH (p:Protein {community_id: cid})
             WHERE p.ec_numbers IS NOT NULL AND size(p.ec_numbers) > 0
             
             // Compte le nombre total de prot√©ines annot√©es dans ce groupe
             WITH cid, count(p) as total_labeled
             
             // Compte la fr√©quence de chaque EC
             MATCH (p:Protein {community_id: cid})
             WHERE p.ec_numbers IS NOT NULL
             UNWIND p.ec_numbers as ec
             WITH cid, total_labeled, ec, count(*) as frequency
             
             // Filtre selon le seuil
             WITH cid, ec, frequency, total_labeled, (toFloat(frequency) / total_labeled) as score
             WHERE score >= $threshold
             
             // Collecte les EC valides
             WITH cid, collect(ec) as valid_ecs
             
             // Mise √† jour des cibles
             MATCH (target:Protein {community_id: cid})
             WHERE target.ec_numbers IS NULL OR size(target.ec_numbers) = 0
             SET target.ec_numbers_calculated = valid_ecs",
            
            {batchSize: 1000, parallel: true, retries: 3, concurrency: 2, params: {threshold: $threshold}}
        )
        YIELD batches, total, errorMessages, committedOperations, retries
        RETURN batches, total, errorMessages, committedOperations, retries
        """

        try:
            with self.driver.session() as session:
                result = session.run(query, threshold=threshold)
                record = result.single()
                if record:
                    print(f"‚úÖ Propagation termin√©e :")
                    print(f"   - Communaut√©s trait√©es : {record['committedOperations']}")
                    print(f"   - Seuil appliqu√© : {threshold}")
        except Exception as e:
            print(f"‚ùå Erreur lors de la mise √† jour pond√©r√©e : {e}")
    
    def get_community_ec_numbers(self, community_id: int, verbose: bool = False) -> List[str]:
        """
        Obtenir les num√©ros EC uniques dans une communaut√© sp√©cifique
        
        Args:
            community_id: ID de la communaut√©
            
        Returns:
            Liste des num√©ros EC uniques
        """
        try:
            with self.driver.session() as session:
                query = """
                MATCH (p:Protein {community_id: $community_id})
                WHERE p.ec_numbers IS NOT NULL
                UNWIND p.ec_numbers AS ec_number
                RETURN DISTINCT ec_number
                ORDER BY ec_number
                """
                
                result = session.run(query, community_id=community_id)
                ec_numbers = [record['ec_number'] for record in result]
                
                if verbose:
                    print(f"‚úÖ {len(ec_numbers)} num√©ros EC dans la communaut√© {community_id}")
                return ec_numbers
                
        except Exception as e:
            print(f"‚ùå Erreur lors de l'obtention des num√©ros EC de la communaut√© : {e}")
            return []
    
    def modify_ec_numbers_per_community(self, community_id: int, new_ec_numbers: List[str]):
        """
        Propager les m√™mes num√©ros EC √† toutes les prot√©ines d'une communaut√© donn√©e
        
        Args:
            community_id: ID de la communaut√©
            new_ec_numbers: Nouvelle liste de num√©ros EC √† attribuer
        """
        try:
            with self.driver.session() as session:
                query = """
                MATCH (p:Protein {community_id: $community_id})
                SET p.ec_numbers_calculated = $new_ec_numbers
                RETURN count(p) AS updated_count
                """
                
                session.run(query, community_id=community_id, new_ec_numbers=new_ec_numbers)
                    
        except Exception as e:
            print(f"‚ùå Erreur lors de la modification des num√©ros EC : {e}")
    
    def update_ec_numbers_from_communities(self):
        """
        Mettre √† jour les num√©ros EC de toutes les prot√©ines en fonction des num√©ros EC de leurs communaut√©s
        """
        # 1) Obtenir le nombre total de communaut√©s
        try:
            with self.driver.session() as session:
                count_query = """
                MATCH (p:Protein)
                WHERE p.community_id IS NOT NULL
                RETURN DISTINCT p.community_id AS communityId
                """
                
                result = session.run(count_query)
                community_ids = [record['communityId'] for record in result]
        except Exception as e:
            print(f"‚ùå Erreur lors de l'obtention des IDs de communaut√© : {e}")
            return
        
        # 2) Pour chaque communaut√©, obtenir les num√©ros EC et les propager
        for community_id in community_ids:
            ec_numbers = self.get_community_ec_numbers(community_id)
            if ec_numbers:
                self.modify_ec_numbers_per_community(community_id, ec_numbers)
        
        print("‚úÖ Mise √† jour des num√©ros EC termin√©e pour toutes les communaut√©s")

    def predict_missing_labels(self, communities_data: List[Dict]) -> Dict[str, Any]:
        """
        Pr√©dire les √©tiquettes bas√©es sur le vote majoritaire dans les communaut√©s
        """
        new_annotations = 0
        details = []

        try:
            # On parcourt les communaut√©s retourn√©es par l'√©tape 1
            for community in communities_data:
                # On ne traite que les communaut√©s mixtes (inconnus + connus)
                if community['unlabeled_proteins'] > 0 and community['unique_ec_numbers'] > 0:
                    
                    # Strat√©gie : Vote Majoritaire
                    # On prend le premier (ou le plus fr√©quent si ta liste est tri√©e)
                    top_label = community['ec_numbers'][0]
                    
                    count_to_update = community['unlabeled_proteins']
                    new_annotations += count_to_update
                    
                    details.append({
                        "community_id": community['community_id'],
                        "predicted_label": top_label,
                        "proteins_affected": count_to_update,
                        "confidence_source": f"Based on {community['labeled_proteins']} labeled neighbors"
                    })
                    
            
            return {
                "total_new_predictions": new_annotations,
                "communities_processed": len(details),
                "predictions_details": details[:10] 
            }
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la pr√©diction : {e}")
            return {"error": str(e)}
        

    def compare_prediction_methods(self, communities_data: List[Dict]) -> Dict[str, Any]:
        """
        Compare les deux m√©thodes (Majorit√© vs Union) pour l'affichage Frontend
        sans √©crire dans la base de donn√©es.
        """
        comparison_results = []
        
        try:
            for community in communities_data:
                # On ne compare que si la communaut√© a des infos (EC numbers) ET des cibles (unlabeled)
                if community['unlabeled_proteins'] > 0 and community['unique_ec_numbers'] > 0:
                    
                    known_ecs = community['ec_numbers'] # Liste des EC pr√©sents dans le groupe
                    
                    # --- ALGO 1 : VOTE MAJORITAIRE (Simulation) ---
                    # C'est une approche "Pr√©cise" mais restrictive
                    algo_majority = known_ecs[0] if known_ecs else "N/A"
                    
                    # --- ALGO 2 : UNION / APOC (Simulation) ---
                    # C'est l'approche "Exhaustive" 
                    algo_union = known_ecs
                    
                    comparison_results.append({
                        "community_id": community['community_id'],
                        "size": community['size'],
                        "nb_known": community['labeled_proteins'],
                        "nb_unknown_targets": community['unlabeled_proteins'],
                        "result_majority": algo_majority,
                        "result_union": algo_union,
                    })
            
            
            return {
                "count": len(comparison_results),
                "data": comparison_results[:50] 
            }
            
        except Exception as e:
            print(f"‚ùå Erreur comparaison : {e}")
            return {"error": str(e)}

    def write_majority_vote(self, communities_data: List[Dict]) -> int:
        """
        √âCRITURE R√âELLE : Applique la logique de Vote Majoritaire en base de donn√©es.
        (Contrairement √† la fonction APOC de ton camarade qui applique l'Union).
        """
        update_count = 0
        try:
            with self.driver.session() as session:
                for community in communities_data:
                    # Conditions : il faut des donn√©es et des cibles
                    if community['unlabeled_proteins'] > 0 and community['unique_ec_numbers'] > 0:
                        
                        # Logique Majorit√© : On prend le premier EC
                        winner_label = community['ec_numbers'][0]
                        community_id = community['community_id']
                        
                        # Requ√™te Cypher pour mettre √† jour
                        # Note : on met winner_label dans une liste [$label] pour garder le format liste
                        query = """
                        MATCH (p:Protein {community_id: $cid})
                        WHERE p.ec_numbers IS NULL OR size(p.ec_numbers) = 0
                        SET p.ec_numbers_calculated = [$label]
                        RETURN count(p) as c
                        """
                        result = session.run(query, cid=community_id, label=winner_label)
                        update_count += result.single()['c']
                        
            print(f"‚úÖ Vote Majoritaire appliqu√© sur {update_count} prot√©ines.")
            return {"committed": update_count, "method": "majority"}            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'√©criture du vote majoritaire : {e}")
            return 0

def demo_community_detection():
    """D√©monstration de la d√©tection de communaut√©s de prot√©ines utilisant LPA"""

    detector = ProteinCommunityDetector(neo4j_uri="bolt://localhost:7687")
    try:
        # Connexion √† Neo4j
        detector.connect()

        print("\n" + "="*80)
        print("TASK 4: D√âTECTION DE COMMUNAUT√âS DE PROT√âINES UTILISANT LA PROPAGATION D'√âTIQUETTES")
        print("="*80)

        # 1. Cr√©ation de la projection de graphe
        print("\n STEP 1: Cr√©ation de la projection de graphe")
        print("-" * 50)
        success = detector.create_graph_projection(min_jaccard_weight=0.1)

        if not success:
            print("‚ùå √âchec de la cr√©ation de la projection de graphe. Sortie.")
            return

        # 2. Estimation de la m√©moire
        print("\nüíæ STEP 2: Estimation de la m√©moire")
        print("-" * 50)
        detector.estimate_lpa_memory()

        # 3. Ex√©cution de LPA avec la configuration par d√©faut
        print("\nüöÄ STEP 3: Ex√©cution de l'algorithme de propagation d'√©tiquettes")
        print("-" * 50)
        lpa_result = detector.run_lpa_community_detection(
            max_iterations=10,
            relationship_weight_property="jaccard_weight",
            min_community_size=2)

        if not lpa_result:
            print("‚ùå √âchec de LPA. Sortie.")
            return
        
        # 4. Propagation des num√©ros EC bas√©s sur les communaut√©s
        print("\nüîÑ STEP 4: Mise √† jour des num√©ros EC bas√©s sur les communaut√©s")
        print("-" * 50)
        detector.update_ec_numbers_weighted()

        # 5. Analyse des communaut√©s
        print("\nüìà STEP 5: Analyse des communaut√©s")
        print("-" * 50)
        analysis = detector.analyze_communities()

        # Afficher les plus grandes communaut√©s
        if analysis and analysis['communities']:
            print(f"\nüèÜ TOP 5 PLUS GRANDES COMMUNAUT√âS:")
            for i, community in enumerate(analysis['communities'][:5]):
                print(f"  {i+1}. Communaut√© {community['community_id']}: "
                      f"{community['size']} prot√©ines "
                      f"(√âtiquet√©es: {community['labeling_rate']:.1%}, "
                      f"Nombres EC: {community['unique_ec_numbers']})")


        # 5. R√©sum√©
        print("\n" + "="*80)
        print("‚úÖ D√âTECTION DE COMMUNAUT√âS TERMIN√âE AVEC SUCC√àS")
        print("="*80)

        print(f"\nüìã R√âSUM√â:")
        if analysis:
            print(f"   - Total communaut√©s d√©tect√©es: {analysis['total_communities']}")
            print(f"   - Prot√©ines dans les communaut√©s: {analysis['total_proteins_in_communities']}")
            print(f"   - Taille moyenne des communaut√©s: {analysis['avg_community_size']:.1f}")
            print(f"   - Plus grande communaut√©: {analysis['largest_community_size']} prot√©ines")
            print(f"   - Taux global d'√©tiquetage dans les communaut√©s: {analysis['overall_labeling_rate']:.1%}")

    except Exception as e:
        print(f"‚ùå √âchec de la d√©monstration : {e}")

    finally:
        # Cleanup
        print(f"\nüßπ CLEANUP:")
        detector.cleanup_projection()
        detector.disconnect()

if __name__ == "__main__":
    demo_community_detection()